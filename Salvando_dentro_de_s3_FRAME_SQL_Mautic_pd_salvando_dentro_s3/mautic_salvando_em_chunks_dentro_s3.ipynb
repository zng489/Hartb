{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "#!pip install fastparquet\n",
    "#!pip install dask\n",
    "import dask.dataframe as dd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install s3contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Session With Boto3.\n",
    "session = boto3.Session(\n",
    "aws_access_key_id='####################',\n",
    "aws_secret_access_key='####################'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://bossa-nova-sss/db-mautic/pdi-test/mktasset_downloads.parquet\n"
     ]
    }
   ],
   "source": [
    "mautic_table = \"mktasset_downloads\"\n",
    "save_table_name = mautic_table+\".parquet\"\n",
    "URI = \"s3://bossa-nova-sss/db-mautic/pdi-test/\"+save_table_name\n",
    "print(URI)\n",
    "#save_table_name = \"mktasset_downloads.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = create_engine('mysql+pymysql://datalake:yb797wdZnigKP!To@private-db3-prd-rds.cyor9py5rdhu.sa-east-1.rds.amazonaws.com/mkt')    \n",
    "conn = cnx.connect()\n",
    "result = conn.execute(f'SELECT * FROM mkt.{mautic_table};').fetchall()\n",
    "df = pd.DataFrame(result)\n",
    "#print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crete and storage in S3 parquet file\n",
    "df.to_parquet(URI, engine='auto', compression='snappy', index=None, partition_cols='id', \n",
    "              storage_options={'key': '####################',\n",
    "              'secret': '####################'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = create_engine('mysql+pymysql://datalake:yb797wdZnigKP!To@private-db3-prd-rds.cyor9py5rdhu.sa-east-1.rds.amazonaws.com/mkt')    \n",
    "conn = cnx.connect()\n",
    "result = conn.execute(f'SELECT * FROM mkt.{mautic_table};').fetchall()\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5834"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOSTRANDO i 2000\n",
      "2000\n",
      "1\n",
      "MOSTRANDO i 4000\n",
      "4000\n",
      "2\n",
      "MOSTRANDO i 6000\n",
      "6000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "chunk = 2000\n",
    "i = 0\n",
    "n = 0\n",
    "while i<= len(df):\n",
    "    # if i greater or equal than len(df) 5834\n",
    "    # 0 <= 5834\n",
    "\n",
    "\n",
    "    ## => The 'i' values was re-defined by this one ******************\n",
    "\n",
    "    j = i + chunk\n",
    "    # then j = 0 + 2000\n",
    "    # j = 2000\n",
    "\n",
    "    # print((i, j))\n",
    "    # print((0, 2000))\n",
    "\n",
    "    tmpdf = df[i:j]\n",
    "    # tmpdf = df[0:2000]\n",
    "\n",
    "    # Here get starts to complicate\n",
    "    i = j\n",
    "    ## => The 'i' values was re-defined by this on ********************\n",
    "    # i = 2000\n",
    "\n",
    "    # print(i)\n",
    "    # print(2000)\n",
    "\n",
    "\n",
    "    print('MOSTRANDO i',i)\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "    n = n + 1\n",
    "    print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = create_engine('mysql+pymysql://datalake:yb797wdZnigKP!To@private-db3-prd-rds.cyor9py5rdhu.sa-east-1.rds.amazonaws.com/mkt')    \n",
    "conn = cnx.connect()\n",
    "result = conn.execute(f'SELECT * FROM mkt.{mautic_table};').fetchall()\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "chunk = 1000\n",
    "i = 0\n",
    "n = 0\n",
    "while i<= len(df):\n",
    "    j = i + chunk\n",
    "    FRAME = df[i:j]\n",
    "\n",
    "    i = j\n",
    "    n = n + 1\n",
    "\n",
    "    FRAME.to_parquet(f\"part.{n}.parquet\", engine='auto', compression='snappy', index=None, partition_cols=None)\n",
    "    #df.to_parquet(f\"part.{n}.parquet\", engine='auto', compression='snappy', index=None, partition_cols=None, \n",
    "    #          storage_options={'key': '####################',\n",
    "    #          'secret': '####################'})\n",
    "    #FRAME.to_parquet(path=f\"./append_data/part.{n}.parquet\",engine='pyarrow', compression='snappy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crete and storage in S3 parquet file\n",
    "df.to_parquet(URI, engine='auto', compression='snappy', index=None, partition_cols='id', \n",
    "              storage_options={'key': '####################',\n",
    "              'secret': '####################'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cunk = 200000\n",
    "    i = 0\n",
    "    n = 0\n",
    "    while i<= len(all_df):\n",
    "        j = i + cunk\n",
    "        print((i, j))\n",
    "        tmpdf = all_df[i:j]\n",
    "        tmpdf.to_parquet(path=f\"./append_data/part.{n}.parquet\",engine='pyarrow', compression='snappy')\n",
    "        i = j\n",
    "        n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Get number of partitions required for nominal 128MB partition size\n",
    "size128MB = int(df.memory_usage().sum()/1e6/128)\n",
    "# Read\n",
    "ddf = dd.from_pandas(df, npartitions=size128MB)\n",
    "save_dir = '/path/to/save/'\n",
    "ddf.to_parquet(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as da\n",
    "\n",
    "ddf = da.from_pandas(df, chunksize=5000000)\n",
    "save_dir = '/path/to/save/'\n",
    "ddf.to_parquet(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov  4 2022, 15:16:59) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10505a93981b783b346019bcd1de71f13ffb9a73d26d578f035aa17159c067ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
